## Confusion Matrix

A confusion matrix is a table that is often used to describe the performance of a classification model (or "classifier") on a set of test data for which the true values are known. The confusion matrix itself is relatively simple to understand, but the related terminology can be confusing.

Let's start with an example confusion matrix for a binary classifier (though it can easily be extended to the case of more than two classes):

#### Example

| n = 165 | Reference (NO)| Reference (YES) |
|:------------- |:------------- |:------------- |
| Predicted (NO) | 50 | 5 |
| Predicted (YES) | 10 | 100 |

What can we learn from this matrix?

- There are two possible predicted classes: "yes" and "no". If we were predicting the presence of a disease, for example, "yes" would mean they have the disease, and "no" would mean they don't have the disease.
- The classifier made a total of 165 predictions (e.g., 165 patients were being tested for the presence of that disease).
- Out of those 165 cases, the classifier predicted "yes" 110 times, and "no" 55 times.
- In reality, 105 patients in the sample have the disease, and 60 patients do not.

Let's now define the most basic terms, which are whole numbers (not rates):

- **true positives (TP)**: These are cases in which we predicted yes (they have the disease), and they do have the disease.
- **true negatives (TN)**: We predicted no, and they don't have the disease.
- **false positives (FP)**: We predicted yes, but they don't actually have the disease. (Also known as a "Type I error.")
- **false negatives (FN)**: We predicted no, but they actually do have the disease. (Also known as a "Type II error.")

I've added these terms to the confusion matrix, and also added the row and column totals:

| n | Reference (NO)| Reference (YES) | Total
|:------------- |:------------- |:------------- |:------------- |
| Predicted (NO) | TN = 50 | FN = 5 | 55 |
| Predicted (YES) | FP = 10 | TP = 100 | 110 |
| Total | 60 | 105 | 165 |

This is a list of rates that are often computed from a confusion matrix for a binary classifier:

- **Accuracy**: Overall, how often is the classifier correct?
	+ (TP+TN)/total = (100+50)/165 = 0.91
- **Misclassification Rate**: Overall, how often is it wrong?
	+ (FP+FN)/total = (10+5)/165 = 0.09
	+ equivalent to 1 minus Accuracy
	+ also known as **"Error Rate"**
	
- **True Positive Rate**: When it's actually yes, how often does it predict yes?
	+ TP/actual yes = 100/105 = 0.95
	+ also known as **"Sensitivity"** or **"Recall"**
	
- **False Positive Rate**: When it's actually no, how often does it predict yes?
	+ FP/actual no = 10/60 = 0.17
	
- **True Negative Rate**: When it's actually no, how often does it predict no?
	+ TN/actual no = 50/60 = 0.83
	+ equivalent to 1 minus False Positive Rate
	+ also known as **"Specificity"**
	
- **Precision**: When it predicts yes, how often is it correct?
	+ TP/predicted yes = 100/110 = 0.91
	
- **Prevalence**: How often does the yes condition actually occur in our sample?
	+ actual yes/total = 105/165 = 0.64
	
#### More Terminology

A couple other terms are also worth mentioning:

- **Null Error Rate**: This is how often you would be wrong if you always predicted the majority class. (In our example, the null error rate would be 60/165=0.36 because if you always predicted yes, you would only be wrong for the 60 "no" cases.) This can be a useful baseline metric to compare your classifier against. However, the best classifier for a particular application will sometimes have a higher error rate than the null error rate, as demonstrated by the [Accuracy Paradox.](https://en.wikipedia.org/wiki/Accuracy_paradox)

- **Cohen's Kappa**: This is essentially a measure of how well the classifier performed as compared to how well it would have performed simply by chance. In other words, a model will have a high Kappa score if there is a big difference between the accuracy and the null error rate. [More details about Cohen's Kappa.](https://en.wikipedia.org/wiki/Cohen's_kappa)
Cohen's Kappa = (Observed Accuracy - Expected Accuracy) / (1 - Expected Accuracy)

- **F Score**: This is a weighted average of the true positive rate (recall) and precision. [More details about the F Score.](https://en.wikipedia.org/wiki/F1_score)
![F1ScoreFormula](TresVista%20Solution/F1ScoreFormula.png "F1 Score")

- **ROC Curve**: This is a commonly used graph that summarizes the performance of a classifier over all possible thresholds. It is generated by plotting the True Positive Rate (y-axis) against the False Positive Rate (x-axis) as you vary the threshold for assigning observations to a given class. [More details about ROC Curves.](https://www.dataschool.io/roc-curves-and-auc-explained/)

**Confidence Interval (CI)**:  It is uesd to describe the amount of uncertainty associated with a sample estimate of a population parameter. The confidence level describes the uncertainty associated with a sampling method.

To express a confidence interval, you need three pieces of information:

- Confidence level: A confidence level refers to the percentage of all possible samples that can be expected to include the true population parameter.
- Statistic: A statistic is a characteristic of a sample. Generally, a statistic is used to estimate the value of a population parameter.
- Margin of error: The margin of error expresses the maximum expected difference between the true population parameter and a sample estimate of that parameter.


**No Information Rate**: The No Information Rate is your best guess given no information beyond the overall distribution of the classes you are trying to predict. In this case, we know from our table that most applicants
(105/165 = 63.64%) did apply for an internal role. So our best guess with no other information is to pick the majority class.

**Mcnemar's Test P-Value**: McNemar's Test (sometimes also called "within-subjects chi-squared test") is a statistical test for paired nominal data. In context of machine learning (or statistical) models, we can use McNemar's Test to compare the predictive accuracy of two models. McNemar's test is based on a 2 times 2 contigency table of the two model's predictions. For more information refer [here](http://rasbt.github.io/mlxtend/user_guide/evaluate/mcnemar/).

**Pos Pred Value**:This is very similar to precision, except that it takes prevalence into account. In the case where the classes are perfectly balanced (meaning the prevalence is 50%), the positive predictive value (PPV) is equivalent to precision. 
	+ (Sensitivity x Prevalence)/((Sensitivity x Prevalence)+ ((1-Specificity) x (1-Prevalence)))

**Neg Pred Value**:
	+ (Specificity x (1-Prevalence))/((1-Sensitivity) x Prevalence)+ ((Specificity) x (1-Prevalence))

**Detection Rate**: It is the proportion of the whole sample where the events were detected correctly. 
	+ So, it is 100 / 165 = 60.61%.

**Prevalence**: Percentage of True YES's in the sample
	+ Prevalence = (FN+TP)/(TN+FP+FN+TP)
	+ So, it is 105 / 165 = 63.64%
	
**Detection Prevalence**: What percentage of the full sample was predicted as YES?
	+ Detection Prevalence = (FP+TP)/(TN+FP+FN+TP)
	+ So, it is 110 / 165 = 66.67%

**Balanced Accuracy**: A balance between correctly predicting the YES's and NO's
	+ Balanced Accuracy = (sensitivity+specificity)/2
	+ So, it is (0.95 + 0.83)/ 2 = 0.89 

### Reference

- [Confusion Matrix Terminology](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)
- [Wikipedia Cohen's_kappa](https://en.wikipedia.org/wiki/Cohen's_kappa)
- [Wikipedia Accuracy_paradox](https://en.wikipedia.org/wiki/Accuracy_paradox)
- [Wikipedia F1_score](https://en.wikipedia.org/wiki/F1_score)
- [Evaluation metrics](https://www.machinelearningplus.com/machine-learning/evaluation-metrics-classification-models-r/)
- [Decoding the Confusion Matrix](https://towardsdatascience.com/decoding-the-confusion-matrix-bb4801decbb)
- [Caret guide](https://topepo.github.io/caret/measuring-performance.html)
- [scaryscientist](http://scaryscientist.blogspot.com/2016/03/confusion-matrix.html)
- [CI](http://www.stat.yale.edu/Courses/1997-98/101/confint.htm)
- [McNemarâ€™s Test](https://machinelearningmastery.com/mcnemars-test-for-machine-learning/)
